---
title: "BGGN_213_class9"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Complete the following code to input the data and store as wisc.df
```{r}
url <- "https://bioboot.github.io/bggn213_S18/class-material/WisconsinCancer.csv"

wisc.df <- read.csv(url)
head(wisc.df)
```
diagnosis cancer vs non-cancer
```{r}
table(wisc.df$diagnosis)
```
# Convert the features of the data: wisc.data, no id or diagnosis
```{r}
#column 33 all NA
wisc.data <- as.matrix(wisc.df[ ,-c(1:2, 33)])

```
# Set the row names of wisc.data
```{r}
row.names(wisc.data) <- wisc.df$id
head(wisc.data)
```
# Create diagnosis vector by completing the missing code
#Q3. How many of the observations have a malignant diagnosis?
```{r}
diagnosis <- as.numeric(wisc.df$diagnosis=="M")
sum(diagnosis)
```
Explore the data you created before (wisc.data and diagnosis) to answer the following questions:

The functions dim(), length(), grep() and sum() may be useful for answering the first 3 questions above.
#Q1. How many observations are in this dataset?
```{r}
#can also look at dim()[1]
nrow(wisc.data)
```
#Q2. How many variables/features in the data are suffixed with _mean?
```{r}
length(grep("_mean", colnames(wisc.data), value=TRUE))

```
```{r}
grep("_mean", colnames(wisc.data), value=TRUE, invert=TRUE)
```
It is important to check if the data need to be scaled before performing PCA. Recall two common reasons for scaling data include:

The input variables use different units of measurement.
The input variables have significantly different variances.
Check the mean and standard deviation of the features (i.e. columns) of the wisc.data to determine if the data should be scaled. Use the  colMeans() and apply() functions like youâ€™ve done before.
# Check column means and standard deviations
```{r}
plot(colMeans(wisc.data), type="h")

```

```{r}
plot(apply(wisc.data,2,sd), type="h")
```
# Perform PCA on wisc.data by completing the following code
```{r}
wisc.pr <- prcomp(wisc.data, scale=TRUE)
summary(wisc.pr)
```
Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?
Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
#PCA result plots
```{r}
plot(wisc.pr$x[ ,1], wisc.pr$x[ ,2], col=diagnosis+1)
```
Calculate the variance of each principal component by squaring the sdev component of wisc.pr (i.e. wisc.pr$sdev^2). Save the result as an object called pr.var.

Calculate the variance explained by each principal component by dividing by the total variance explained of all principal components. Assign this to a variable called pve and create a plot of variance explained for each principal component.
```{r}
# Variance explained by each principal component: pve
pve <- wisc.pr$sdev^2/sum(wisc.pr$sdev^2)
# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```
#barplot version
```{r}
barplot(pve, names.arg=paste("PC", 1:length(pve)), las=2, axes=FALSE, ylab="Proportion of Variance")
axis(2, at=pve, labels=round(pve,2)*100 )
```
#hiearchial clustering
# Scale the wisc.data data: data.scaled
```{r}
data.scaled <- scale(wisc.data)
data.dist <- dist(data.scaled) #default= method = "euclidean"
wisc.hclust <- hclust(data.dist) #default= method="complete"

plot(wisc.hclust)
abline(h=20, col="red", lwd=3)
```

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)

table(wisc.hclust.clusters, diagnosis)
```
#kmeans
Create a k-means model on wisc.data, assigning the result to wisc.km. Be sure to create 2 clusters, corresponding to the actual number of diagnosis. Also, remember to scale the data (with the scale() function and repeat the algorithm 20 times (by setting setting the value of the  nstart argument appropriately). Running multiple times such as this will help to find a well performing model.
```{r}
data.scaled <- scale(wisc.data)
wisc.km <- kmeans(data.scaled, centers=2, nstart=20)

table(wisc.km$cluster)
```
compare to expert diagnosis
```{r}
table(wisc.km$cluster, diagnosis)
```
#USing PCA info for clustering
Using the minimum number of principal components required to describe at least 90% of the variability in the data, create a hierarchical clustering model with complete linkage. Assign the results to wisc.pr.hclust.

## Use the distance along the first 3 PCs for clustering i.e. wisc.pr$x[, 1:3]
```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:3]), method="ward.D2")

plot(wisc.pr.hclust)
```

```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)

plot(wisc.pr$x[,1:2], col=wisc.pr.hclust.clusters)
```
```{r}
table(wisc.pr.hclust.clusters, diagnosis)
```
#Test Model
```{r}
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
```
```{r}
npc <- predict(wisc.pr, newdata=new)
plot(wisc.pr$x[,1:2], col=wisc.pr.hclust.clusters)
points(npc[,1], npc[,2], col=c("purple", "blue"), pch=16, cex=3)
```


